\documentclass[11pt]{article}

% packages
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
%\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{lastpage}
\usepackage{units}
\usepackage[margin=0.75in]{geometry}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{sectsty}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{listings}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{multirow}

\definecolor{sblue}{HTML}{5292c0}
\definecolor{sgreen}{HTML}{93c47d}
\definecolor{sorange}{HTML}{e69138}
\definecolor{codeblue}{rgb}{0.29296875, 0.51953125, 0.68359375}
\definecolor{codegreen}{rgb}{0.47265625, 0.62890625, 0.40234375}
\definecolor{codegray}{rgb}{0.95703125, 0.95703125, 0.95703125}
\definecolor{codecrimson}{rgb}{0.87109375,0.3984375,0.3984375}

\lstset{frame=tb,
  backgroundcolor=\color{codegray},
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{codeblue},
  commentstyle=\color{codegreen},
  stringstyle=\color{codecrimson},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4,
  frame=tlbr,framesep=4pt,framerule=0pt,
  literate={`}{\`}1,
}

% colors
\definecolor{sblue}{HTML}{5292c0}
\definecolor{sgreen}{HTML}{93c47d}
\definecolor{sorange}{HTML}{e69138}

% sectioning magic
\counterwithin*{equation}{section}
%\numberwithin{equation}{section}

% fonts
\usepackage{fontspec}
\newfontfamily\headerfontlt{ITC Franklin Gothic Std Book}
\newfontfamily\headerfont{ITC Franklin Gothic Std Demi}
\usepackage[urw-garamond]{mathdesign}
\usepackage{garamondx}
\usepackage[italic]{mathastext}

\newcommand{\printsection}[1]{\normalfont\headerfontlt{{{#1}}}}
\newcommand{\printsubsection}[1]{\normalfont\headerfontlt\textcolor{darkgray}{{#1}}}
\newcommand{\printsubsubsection}[1]{\normalfont\headerfontlt{{#1}}}
\allsectionsfont{\printsection}
\subsectionfont{\printsection}
\subsectionfont{\printsubsection}
\subsubsectionfont{\printsubsubsection}


\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{gray}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\op}[1]{\textrm{\small\printsubsection{\MakeUppercase{#1}}}\,}
\newcommand{\opns}[1]{\textrm{\small\printsubsection{\MakeUppercase{#1}}}}
\newcommand{\Partial}[1]{\partial\hspace{-0.2ex}{#1}}
\newcommand{\D}[1]{\mathrm{d}#1}
\newcommand{\E}[1]{\mathbb{E}{\left[\,#1\,\right]}}
\newcommand{\var}[1]{\mathrm{var}\left( #1 \right)}
\newcommand{\cov}[1]{\mathrm{cov}\left( #1 \right)}
\newcommand{\pr}[1]{\mathrm{Pr}\left( #1 \right)}
\newcommand{\given}{\,|\,}
\renewcommand{\det}{\mathrm{det\,}}
\renewcommand{\det}[1]{\op{det}\left(#1\right)}
\renewcommand{\ker}{\mathrm{Ker\,}}
\newcommand{\trace}[1]{\op{trace}\left(#1\right)}
\newcommand{\nul}[1]{\op{nul}\left(#1\right)}
\newcommand{\col}[1]{\op{col}\left(#1\right)}
\newcommand{\row}[1]{\op{row}\left(#1\right)}
\newcommand{\rank}[1]{\op{rank}\left(#1\right)}
\renewcommand{\dim}[1]{\op{dim}\left(#1\right)}
\newcommand{\im}{\op{Im\,}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand*{\matr}[1]{\mathbfit{#1}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\conj}[1]{\overline{#1}}
\newcommand*{\hermconj}{^{\mathsf{H}}}
\newcommand*{\inv}{^{-1}}

%\setlength\parindent{0pt}
\linespread{1.1}
\pagecolor{gray!1}


% header/footer
\topmargin=-0.65in
\pagestyle{fancy}\fancyhf{} 
\lhead{\headerfontlt{\textcolor{darkgray}{Satej Soman \textcolor{gray}{$\big/$} CAPP30254, Spring 19 \textcolor{gray}{$\big/$}} Homework 3}}
\rhead{\headerfontlt{\textcolor{gray}{$\big[$} \thepage\ \textcolor{gray}{$\big/$} \pageref{LastPage} \textcolor{gray}{$\big]$}}}

\begin{document}
\begin{titlepage}
\raggedleft\huge\headerfontlt{
\textcolor{darkgray}{Satej Soman\\
CAPP30254: Machine Learning for Public Policy\\
Spring 2019}}

\vspace{240pt}
\Huge\headerfontlt{\textcolor{darkgray}{HW 3\\MACHINE LEARNING PIPELINE\\IMPROVEMENTS \& EVALUATION}}
\vfill
\normalfont \normalsize
\tableofcontents

\end{titlepage}
\section*{Notes}
\begin{itemize}
\item Representative code snippets are interspersed with analysis and explanations below; all code is available on GitHub: \url{https://github.com/satejsoman/capp30254/tree/master/hw3}.
\item The pipeline library lives in the \texttt{code/pipeline} directory, while the sample application which imports the library is \texttt{code/donors\_choose.py}.
\end{itemize}

\section{Coding: Pipeline Improvements}
\subsection{Improvements}
The following improvements have been made to the \texttt{pipeline} library:
\begin{itemize}
\item The library now includes a stage to generate train/test data splits. Due to the object-oriented design, the current implementation can easily be overriden by subclassing or monkey-patching in specific applications.
\item Additional metrics, besides accuracy, have been added to the model evaluation. Specifically, precision, recall, and ROC-AUC have been added to the model evaluation stage.
\end{itemize}

\subsection{Additional Models}
In the original pipeline design, the model was not hard-coded, so pipeline runs can be parametrized by the model implementation: 

\begin{lstlisting}[language=Python,numbers=none]
donors_choose_preprocessors = [
    ...
]
donors_choose_feature_generators = [
    ...
]

models_to_run = { 
    ...
}

def model_parametrized_pipeline(description, model):
    return Pipeline(input_path, "funded_in_60_days",
        summarize=False,
        data_preprocessors=donors_choose_preprocessors,
        feature_generators=donors_choose_feature_generators,
        name="donors-choose-" + description,
        model=model
        output_root_dir="output")

for (description, model) in models_to_run.items():
    model_parametrized_pipeline(description, model).run()
\end{lstlisting}

The above code is purely representative; in implementation, the data generation and feature preprocessing are done in a separate pipeline. The results of this pipeline are serialized and fed in as the inputs to a pipeline that solely trains and tests models. In this way, computation to process data and create feature vectors is not repeated for every trial run.


\section{Analysis: Application to DonorsChoose Project Funding Viability}

\subsection{Background}
DonorsChoose.org is a platform for soliciting donations for K-12 schools in need. Teachers post a project proposal highlighting a shortfall of resources at their institution as well as information about what areas of study will benefit from the project meeting its funding requirements. Additional data available on DonorsChoose.org describe the socioeconomic status and geography of the school, as well as characteristics of the teacher. 

Responding to shortfalls in educational resources is a time-sensitive matter; donors and teachers should therefore understand the factors that determine whether a project will succeed within a reasonable amount of time (in this analysis, 60 days). Correctly predicting which projects are viable allows for effective use of time and resources for both the donors and the schools. 

\subsection{Data Exploration}
Preliminary inspection of the data identifies columns that should be excluded from further analysis. Project- and school-specific identifiers such as \texttt{projectid},  \texttt{school\_ncesid}, and \texttt{schoolid} are not useful for predicting funding success. Additionally, we choose to \textit{exclude} the teacher's preferred prefix (\texttt{teacher\_prefix}) because it proxies stated gender, a protected class under Title VII. 

Table \ref{summary} shows summary statistics for other numerical inputs. While latitude and longitude are potentially good predictors, geographic variation can be captured by categorical variables such as \texttt{school\_city}, \texttt{school\_state}, and \texttt{school\_district}. 

Further, we can see that two other predictors, the number of students reached and the total price, need to be scaled since many of the other predictors are binary. Without scaling, the use of these predictors will degrade SVM performance, and distort results in $k$-nearest neighbors analysis.

Finally, we also see that 71\% of projects are funded within 60 days. A good baseline classifier would one that randomly predicts success with a probability of 71\%.
\begin{table}[H]
\centering \small \renewcommand{\arraystretch}{1.1}
\input{summary.tex}
\caption{Table of summary statistics for additional numerical vectors.}\label{summary}
\end{table}

Additionally, other input vectors have missing values. Depending on the number of missing values and the domain context, we can fill in missing values, or exclude overly sparse vectors from our models. Table \ref{missing} shows that variables such as the schools metropolitan area and the projects secondary focus are likely too sparse to support filling with placeholder values, whereas appropriate missing values for resource type, grade level, and students reached can be found by examining the distributions for those variables. As an example, Figure \ref{sr_dist} shows the distribution of \texttt{students\_reached} - downcoding missing values to 0 is an appropriate approximation for the number of students reached and will not materially change the distribution.

\begin{minipage}[c]{0.3\pagewidth}
\vspace{0pt}
\begin{table}[H]
\centering \small \renewcommand{\arraystretch}{1.1}
\input{missing.tex}
\caption{Vectors with missing values.}\label{missing}
\end{table}
\end{minipage} \hfill \begin{minipage}[c]{0.45\pagewidth}
\vspace{0pt}
\begin{figure}[H]
\centering
\input{students_reached.tex}
\caption{Distribution of number of students reached.} \label{sr_dist}
\end{figure}
\end{minipage}

\subsection{Feature Selection}
As discussed above, we exclude teacher characteristics but focus on project-level, school-specific, and temporal variables. Table \ref{features} lists the chosen features and provides a summary of each. 
\begin{table}[H]
\centering \small \renewcommand{\arraystretch}{1.1}
\input{features.tex}
\caption{Features passed to machine learning models.}\label{features}
\end{table}

\subsection{Classifier Models}
A number of classifiers are used to try and predict which projects on DonorsChoose will be funded. The implementations available through scikit-learn offer a number of parameters influencing the performance of the classifier. 

\begin{table}[H]
\centering \small \renewcommand{\arraystretch}{1.1}
\input{classifiers.tex}
\caption{Classifiers and parameters tested.}\label{classifiers}
\end{table}


\section{Report: Classifier Performance}

\end{document} 
