\documentclass[11pt]{article}

% packages
\usepackage{enumerate}
\usepackage{fancyhdr}
\usepackage{extramarks}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
%\usepackage{amsfonts}
\usepackage{tikz}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{lastpage}
\usepackage{units}
\usepackage[margin=0.75in]{geometry}
\usepackage{pgfplots}
\pgfplotsset{compat=1.16}
\usepackage{sectsty}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{matrix}
\usepackage{listings}
\usepackage[plain]{algorithm}
\usepackage{algpseudocode}
\usepackage{mdframed}
\usepackage{booktabs}
\usepackage{multirow}

\definecolor{sblue}{HTML}{5292c0}
\definecolor{sgreen}{HTML}{93c47d}
\definecolor{sorange}{HTML}{e69138}
\definecolor{codeblue}{rgb}{0.29296875, 0.51953125, 0.68359375}
\definecolor{codegreen}{rgb}{0.47265625, 0.62890625, 0.40234375}
\definecolor{codegray}{rgb}{0.95703125, 0.95703125, 0.95703125}
\definecolor{codecrimson}{rgb}{0.87109375,0.3984375,0.3984375}

\lstset{frame=tb,
  backgroundcolor=\color{codegray},
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=left,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{codeblue},
  commentstyle=\color{codegreen},
  stringstyle=\color{codecrimson},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=4,
  frame=tlbr,framesep=4pt,framerule=0pt,
  literate={`}{\`}1,
}

% colors
\definecolor{sblue}{HTML}{5292c0}
\definecolor{sgreen}{HTML}{93c47d}
\definecolor{sorange}{HTML}{e69138}

% sectioning magic
\counterwithin*{equation}{section}
%\numberwithin{equation}{section}

% fonts
\usepackage{fontspec}
\newfontfamily\headerfontlt{ITC Franklin Gothic Std Book}
\newfontfamily\headerfont{ITC Franklin Gothic Std Demi}
\usepackage[urw-garamond]{mathdesign}
\usepackage{garamondx}
\usepackage[italic]{mathastext}

\newcommand{\printsection}[1]{\normalfont\headerfontlt{{{#1}}}}
\newcommand{\printsubsection}[1]{\normalfont\headerfontlt\textcolor{darkgray}{{#1}}}
\newcommand{\printsubsubsection}[1]{\normalfont\headerfontlt{{#1}}}
\allsectionsfont{\printsection}
\subsectionfont{\printsection}
\subsectionfont{\printsubsection}
\subsubsectionfont{\printsubsubsection}


\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\headrule}{\hbox to\headwidth{\color{gray}\leaders\hrule height \headrulewidth\hfill}}
\renewcommand{\footrulewidth}{0.0pt}

\newcommand{\op}[1]{\textrm{\small\printsubsection{\MakeUppercase{#1}}}\,}
\newcommand{\opns}[1]{\textrm{\small\printsubsection{\MakeUppercase{#1}}}}
\newcommand{\Partial}[1]{\partial\hspace{-0.2ex}{#1}}
\newcommand{\D}[1]{\mathrm{d}#1}
\newcommand{\E}[1]{\mathbb{E}{\left[\,#1\,\right]}}
\newcommand{\var}[1]{\mathrm{var}\left( #1 \right)}
\newcommand{\cov}[1]{\mathrm{cov}\left( #1 \right)}
\newcommand{\pr}[1]{\mathrm{Pr}\left( #1 \right)}
\newcommand{\given}{\,|\,}
\renewcommand{\det}{\mathrm{det\,}}
\renewcommand{\det}[1]{\op{det}\left(#1\right)}
\renewcommand{\ker}{\mathrm{Ker\,}}
\newcommand{\trace}[1]{\op{trace}\left(#1\right)}
\newcommand{\nul}[1]{\op{nul}\left(#1\right)}
\newcommand{\col}[1]{\op{col}\left(#1\right)}
\newcommand{\row}[1]{\op{row}\left(#1\right)}
\newcommand{\rank}[1]{\op{rank}\left(#1\right)}
\renewcommand{\dim}[1]{\op{dim}\left(#1\right)}
\newcommand{\im}{\op{Im\,}}
\newcommand{\reals}{\mathbb{R}}
\newcommand{\complex}{\mathbb{C}}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand*{\matr}[1]{\mathbfit{#1}}
\newcommand*{\tran}{^{\mkern-1.5mu\mathsf{T}}}
\newcommand*{\conj}[1]{\overline{#1}}
\newcommand*{\hermconj}{^{\mathsf{H}}}
\newcommand*{\inv}{^{-1}}

%\setlength\parindent{0pt}
%\linespread{1.1}
\pagecolor{gray!1}


% header/footer
\topmargin=-0.65in
\pagestyle{fancy}\fancyhf{} 
\lhead{\headerfontlt{\textcolor{darkgray}{Satej Soman \textcolor{gray}{$\big/$} CAPP30254, Spring 19 \textcolor{gray}{$\big/$}} Homework 5}}
\rhead{\headerfontlt{\textcolor{gray}{$\big[$} \thepage\ \textcolor{gray}{$\big/$} \pageref{LastPage} \textcolor{gray}{$\big]$}}}

\begin{document}
\begin{titlepage}
\raggedleft\huge\headerfontlt{
\textcolor{darkgray}{Satej Soman\\
CAPP30254: Machine Learning for Public Policy\\
Spring 2019}}

\vspace{240pt}
\Huge\headerfontlt{\textcolor{darkgray}{HW 5\\MACHINE LEARNING PIPELINE\\ADDITIONAL IMPROVEMENTS}}
\vfill
\normalfont \normalsize
\tableofcontents

\end{titlepage}
\section*{Notes}
\begin{itemize}
\item Representative code snippets are interspersed with analysis and explanations below; all code is available on GitHub: \url{https://github.com/satejsoman/capp30254/tree/master/hw5}.
\item The pipeline library lives in the \texttt{code/pipeline} directory, while the sample application which imports the library is \texttt{code/donors\_choose.py}.
\item The generated table of hyperparameters is uploaded as a separate CSV; it also is available on GitHub as \texttt{code/output/donors\_choose-LATEST/evaluations.csv}.
\end{itemize}

\section{Pipeline Improvements}
The following improvements have been made to the \texttt{pipeline} library:
\subsection{Code Structure}
\begin{itemize}
\item \textbf{hyperparameter grid}: A concept of a hyperparameter grid is formalized in the \texttt{pipeline.Grid} class. For each model, the parameters are varied, and the \texttt{Grid} class ensures the Cartesian product of all parameter values is trained and tested.

Example configuration: 
\begin{lstlisting}[numbers=none]
models:
  LogisticRegression:
    C: [0.01, 0.1, 1, 10, 100]
    penalty: ["l1", "l2"]
    n_jobs: [-1]

  KNeighborsClassifier:
    n_neighbors: [10, 50, 100]
    n_jobs: [-1]

  DecisionTreeClassifier:
    max_depth: [null, 1, 5, 10, 50, 100]
\end{lstlisting}

\item \textbf{extended configuration}: Most pipeline settings are now read from a \texttt{config.yml} file instead of being hard-coded.
\item \textbf{separate imputation for test and training sets}: Imputation of missing values is done per  training and testing dataset instead of on the entire dataset. 
\item \textbf{metrics at \textit{k}}: Metrics are calculated by varying the top $k$\% of scored entities rather than by thresholding. See \texttt{code/pipeline/evaluation.py} for implementation.
\item \textbf{unit tests}: Unit tests are located at \texttt{code/pipeline/tests}. By running these tests (using either \texttt{pytest} or \texttt{python -m unittest discover}), we can confirm that: 
\begin{itemize}
\item the imputation of missing values runs only on each training and testing set 
\item the splitting of datasets matches the specification in the pipeline configuration
\item a sample pipeline is working as a smoketest for any new changes made to the \texttt{pipeline} library
\end{itemize} 
\end{itemize}

\subsection{Methodology}
\begin{itemize}
\item \textbf{training labels}: The label for the analyzed DataChoose data set has been updated so it a label of \texttt{1} indicates a project was \textit{not} funded within 60 days.
\item \textbf{test/train buffers}: There are 60 day gaps between the end of each training set and the beginning of its corresponding test set to account for the predicted outcome occurring after the end of the training period.
\end{itemize}

\subsection{Models}
\begin{itemize}
\item \textbf{SVM}: A linear support-vector machine is included in the models trained by the pipeline.
\item \textbf{meaningful parameters}: Decision tree classifiers are now run varying maximum tree depth instead of information content measure, and the size of ensemble in random forests is also varied. 
\end{itemize}

\pagebreak
\section{Analysis}
Please see Homework 3 for initial analysis and data exploration. The following analysis focuses on new results as a result of pipeline changes. As a brief summary, Table \ref{features} lists the features passed to the classifiers, and Table \ref{classifiers} lists the classifiers tested (including information on which parameters were controlled).

\begin{table}[H]
\centering \footnotesize  \renewcommand{\arraystretch}{1.1}
\input{features.tex}
\caption{Features passed to machine learning models.}\label{features}
\end{table}

\begin{table}[H]
\centering \footnotesize  \renewcommand{\arraystretch}{1.1}
\input{classifiers.tex}
\caption{Classifiers and parameters tested.}\label{classifiers}
\end{table}

With the label properly calculated (i.e. \texttt{1} indicates failure to receive funding), we can answer the following questions: 

\begin{itemize}
\item \textbf{Which classifier does better on which metrics?}

Across both precision and recall for at the top 1\%, 2\% and 5\%, the SVM and logistic regression classifier families outperform the other types of classifiers. This is not surprising, as the approach taken here resembles the causal inference worldview, where a small number of features is sufficient. Creating hundreds of features would allow the decision tree-based classifiers to perform much better.

\item \textbf{How do results change over time?}

Almost all classifiers show increasing performance over time; this is understandable, as models trained on the larger test sets should have higher performance since they are able to see more data points in the training phase. 
\item \textbf{What would be your recommendation to someone who's working on this model to identify 5\% of posted projects that are at highest risk of not getting fully funded to intervene with, which model should they decide to go forward with and deploy?} 

I would recommend a support-vector machine with an $L2$ penalty and further investigation to find optimal cost. While some classifier families perform strictly better when measured by precision at 5\%, SVM classifiers perform the best when measured by precision at 2\%. Presumably, the budget for intervention would be spent first on the projects at highest risk of failure. The higher precision of the SVM classifiers at 2\% of the project population indicates ranking by the scores from this model would lead to the most effective interventions. Figure \ref{pr} shows the precision-recall curve for an SVM across all three test sets.

\begin{figure}[H]
\centering \label{pr}
\input{pr-curve.tex}
\caption{The precision-recall curve across the whole project population for a sample SVM classifier, evaluated on all three validation sets.}
\end{figure}


\end{itemize}

\end{document} 
